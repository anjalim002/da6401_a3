{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11874346,"sourceType":"datasetVersion","datasetId":7462499}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Import necessary libraries\nimport os\nimport pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nimport wandb\nfrom tqdm import tqdm\nimport random\nimport matplotlib.pyplot as plt\nfrom collections import Counter\n\nimport wandb\nwandb.login(key=\"5377a200ae8c04c015415319969d0f2ea19c027c\")\n\n# Check if GPU is available\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\")\n\n# Data loading and preprocessing\n# def load_data(file_path):\n#     \"\"\"Load data from TSV file\"\"\"\n#     df = pd.read_csv(file_path, sep='\\t', header=None)\n#     df.columns = ['latin', 'native']\n#     return df\n# Data loading and preprocessing\ndef load_data(file_path):\n    \"\"\"Load data from TSV file\"\"\"\n    df = pd.read_csv(file_path, sep='\\t', header=None, usecols=[0, 1])\n    df.columns = ['native', 'latin']\n    return df\n\n\n# Load data files\ntrain_data = load_data('/kaggle/input/hindi-dl/hi.translit.sampled.train.tsv')\nval_data = load_data('/kaggle/input/hindi-dl/hi.translit.sampled.dev.tsv')\ntest_data = load_data('/kaggle/input/hindi-dl/hi.translit.sampled.test.tsv')\n\nprint(f\"Train data size: {len(train_data)}\")\nprint(f\"Validation data size: {len(val_data)}\")\nprint(f\"Test data size: {len(test_data)}\")\n\n# Display first few examples\nprint(\"\\nFirst 5 examples from training data:\")\nprint(train_data.head())","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-20T16:23:25.775168Z","iopub.execute_input":"2025-05-20T16:23:25.775890Z","iopub.status.idle":"2025-05-20T16:23:25.881742Z","shell.execute_reply.started":"2025-05-20T16:23:25.775864Z","shell.execute_reply":"2025-05-20T16:23:25.881040Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create vocabulary and tokenizer classes\nclass Vocabulary:\n    def __init__(self, freq_threshold=0):\n        # Special tokens\n        self.itos = {0: \"<PAD>\", 1: \"<SOS>\", 2: \"<EOS>\", 3: \"<UNK>\"}\n        self.stoi = {\"<PAD>\": 0, \"<SOS>\": 1, \"<EOS>\": 2, \"<UNK>\": 3}\n        self.freq_threshold = freq_threshold\n        self.idx = 4  # Starting index for new tokens\n        \n    def __len__(self):\n        return len(self.itos)\n    \n    def build_vocabulary(self, text_list):\n        \"\"\"Build vocabulary from list of texts\"\"\"\n        frequencies = Counter()\n        for text in text_list:\n            for char in text:\n                frequencies[char] += 1\n        \n        # Add tokens that meet frequency threshold\n        for char, freq in frequencies.items():\n            if freq >= self.freq_threshold:\n                self.stoi[char] = self.idx\n                self.itos[self.idx] = char\n                self.idx += 1\n    \n    def numericalize(self, text):\n        \"\"\"Convert text to list of indices\"\"\"\n        numericalized = []\n        for char in text:\n            if char in self.stoi:\n                numericalized.append(self.stoi[char])\n            else:\n                numericalized.append(self.stoi[\"<UNK>\"])\n        return numericalized\n    \n    def decode(self, indices):\n        \"\"\"Convert indices to text\"\"\"\n        return ''.join([self.itos[idx] for idx in indices if idx not in [0, 1, 2, 3]])\n    \n# Create source and target vocabularies\nsrc_vocab = Vocabulary()\ntgt_vocab = Vocabulary()\n\ntrain_data = train_data.dropna()\n# Build vocabularies\nsrc_vocab.build_vocabulary(train_data['latin'].tolist())\ntgt_vocab.build_vocabulary(train_data['native'].tolist())\n\nprint(f\"Source vocabulary size: {len(src_vocab)}\")\nprint(f\"Target vocabulary size: {len(tgt_vocab)}\")\n\n# Create dataset class\nclass TransliterationDataset(Dataset):\n    def __init__(self, df, src_vocab, tgt_vocab):\n        self.df = df\n        self.src_vocab = src_vocab\n        self.tgt_vocab = tgt_vocab\n        \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, index):\n        src_text = self.df.iloc[index]['latin']\n        tgt_text = self.df.iloc[index]['native']\n        \n        # Add SOS and EOS tokens to target\n        src_numericalized = [self.src_vocab.stoi[\"<SOS>\"]] + \\\n                         self.src_vocab.numericalize(src_text) + \\\n                         [self.src_vocab.stoi[\"<EOS>\"]]\n        \n        tgt_numericalized = [self.tgt_vocab.stoi[\"<SOS>\"]] + \\\n                          self.tgt_vocab.numericalize(tgt_text) + \\\n                          [self.tgt_vocab.stoi[\"<EOS>\"]]\n        \n        return {\n            \"src_text\": src_text,\n            \"tgt_text\": tgt_text,\n            \"src\": torch.tensor(src_numericalized),\n            \"tgt\": torch.tensor(tgt_numericalized)\n        }\n\n# Create padding collate function for batching\ndef pad_collate(batch):\n    src_lens = [len(item[\"src\"]) for item in batch]\n    tgt_lens = [len(item[\"tgt\"]) for item in batch]\n    \n    max_src_len = max(src_lens)\n    max_tgt_len = max(tgt_lens)\n    \n    padded_src = torch.zeros(len(batch), max_src_len).long()\n    padded_tgt = torch.zeros(len(batch), max_tgt_len).long()\n    \n    for i, item in enumerate(batch):\n        src_len = len(item[\"src\"])\n        tgt_len = len(item[\"tgt\"])\n        \n        padded_src[i, :src_len] = item[\"src\"]\n        padded_tgt[i, :tgt_len] = item[\"tgt\"]\n    \n    src_texts = [item[\"src_text\"] for item in batch]\n    tgt_texts = [item[\"tgt_text\"] for item in batch]\n    \n    return {\n        \"src_texts\": src_texts,\n        \"tgt_texts\": tgt_texts,\n        \"src\": padded_src,\n        \"tgt\": padded_tgt,\n        \"src_lens\": torch.tensor(src_lens),\n        \"tgt_lens\": torch.tensor(tgt_lens)\n    }\n\n# Create datasets\ntrain_dataset = TransliterationDataset(train_data, src_vocab, tgt_vocab)\nval_dataset = TransliterationDataset(val_data, src_vocab, tgt_vocab)\ntest_dataset = TransliterationDataset(test_data, src_vocab, tgt_vocab)\n\n# Test the dataset\nsample = train_dataset[0]\nprint(f\"\\nSample from dataset:\")\nprint(f\"Source text: {sample['src_text']}\")\nprint(f\"Target text: {sample['tgt_text']}\")\nprint(f\"Source indices: {sample['src']}\")\nprint(f\"Target indices: {sample['tgt']}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T16:25:41.628428Z","iopub.execute_input":"2025-05-20T16:25:41.628701Z","iopub.status.idle":"2025-05-20T16:25:41.809116Z","shell.execute_reply.started":"2025-05-20T16:25:41.628683Z","shell.execute_reply":"2025-05-20T16:25:41.808529Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Encoder model\nclass Encoder(nn.Module):\n    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers, dropout, cell_type):\n        super(Encoder, self).__init__()\n        self.hidden_dim = hidden_dim\n        self.num_layers = num_layers\n        self.cell_type = cell_type\n        \n        # Embedding layer\n        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n        \n        # RNN layer (RNN, GRU, or LSTM)\n        if cell_type == 'RNN':\n            self.rnn = nn.RNN(\n                embedding_dim, \n                hidden_dim, \n                num_layers=num_layers, \n                dropout=dropout if num_layers > 1 else 0,\n                batch_first=True\n            )\n        elif cell_type == 'GRU':\n            self.rnn = nn.GRU(\n                embedding_dim, \n                hidden_dim, \n                num_layers=num_layers, \n                dropout=dropout if num_layers > 1 else 0,\n                batch_first=True\n            )\n        elif cell_type == 'LSTM':\n            self.rnn = nn.LSTM(\n                embedding_dim, \n                hidden_dim, \n                num_layers=num_layers, \n                dropout=dropout if num_layers > 1 else 0,\n                batch_first=True\n            )\n        else:\n            raise ValueError(\"cell_type must be one of ['RNN', 'GRU', 'LSTM']\")\n        \n        self.dropout = nn.Dropout(dropout)\n        \n    def forward(self, x):\n        # x: [batch_size, seq_len]\n        \n        # Pass through embedding\n        embedded = self.dropout(self.embedding(x))\n        # embedded: [batch_size, seq_len, embedding_dim]\n        \n        # Pass through RNN\n        if self.cell_type == 'LSTM':\n            outputs, (hidden, cell) = self.rnn(embedded)\n            # hidden: [num_layers, batch_size, hidden_dim]\n            # cell: [num_layers, batch_size, hidden_dim]\n            return outputs, (hidden, cell)\n        else:  # RNN or GRU\n            outputs, hidden = self.rnn(embedded)\n            # hidden: [num_layers, batch_size, hidden_dim]\n            return outputs, hidden\n\n# Decoder model\nclass Decoder(nn.Module):\n    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers, dropout, cell_type):\n        super(Decoder, self).__init__()\n        self.vocab_size = vocab_size\n        self.hidden_dim = hidden_dim\n        self.num_layers = num_layers\n        self.cell_type = cell_type\n        \n        # Embedding layer\n        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n        \n        # RNN layer (RNN, GRU, or LSTM)\n        if cell_type == 'RNN':\n            self.rnn = nn.RNN(\n                embedding_dim, \n                hidden_dim, \n                num_layers=num_layers, \n                dropout=dropout if num_layers > 1 else 0,\n                batch_first=True\n            )\n        elif cell_type == 'GRU':\n            self.rnn = nn.GRU(\n                embedding_dim, \n                hidden_dim, \n                num_layers=num_layers, \n                dropout=dropout if num_layers > 1 else 0,\n                batch_first=True\n            )\n        elif cell_type == 'LSTM':\n            self.rnn = nn.LSTM(\n                embedding_dim, \n                hidden_dim, \n                num_layers=num_layers, \n                dropout=dropout if num_layers > 1 else 0,\n                batch_first=True\n            )\n        else:\n            raise ValueError(\"cell_type must be one of ['RNN', 'GRU', 'LSTM']\")\n        \n        # Output layer\n        self.fc_out = nn.Linear(hidden_dim, vocab_size)\n        self.dropout = nn.Dropout(dropout)\n        \n    def forward(self, x, hidden):\n        # x: [batch_size, 1]\n        # hidden: [num_layers, batch_size, hidden_dim] for RNN/GRU\n        # or tuple of ([num_layers, batch_size, hidden_dim], [num_layers, batch_size, hidden_dim]) for LSTM\n        \n        # Pass through embedding\n        embedded = self.dropout(self.embedding(x))\n        # embedded: [batch_size, 1, embedding_dim]\n        \n        # Pass through RNN\n        if self.cell_type == 'LSTM':\n            output, (hidden, cell) = self.rnn(embedded, hidden)\n            # output: [batch_size, 1, hidden_dim]\n            # hidden: [num_layers, batch_size, hidden_dim]\n            # cell: [num_layers, batch_size, hidden_dim]\n        else:  # RNN or GRU\n            output, hidden = self.rnn(embedded, hidden)\n            # output: [batch_size, 1, hidden_dim]\n            # hidden: [num_layers, batch_size, hidden_dim]\n        \n        # Pass through output layer\n        prediction = self.fc_out(output.squeeze(1))\n        # prediction: [batch_size, vocab_size]\n        \n        if self.cell_type == 'LSTM':\n            return prediction, (hidden, cell)\n        else:\n            return prediction, hidden\n\n# Seq2Seq model\nclass Seq2Seq(nn.Module):\n    def __init__(self, encoder, decoder, device):\n        super(Seq2Seq, self).__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n        self.device = device\n        \n        # Ensure encoder and decoder have same dimensions and cell types\n        assert encoder.hidden_dim == decoder.hidden_dim\n        assert encoder.num_layers == decoder.num_layers\n        assert encoder.cell_type == decoder.cell_type\n        \n    def forward(self, src, tgt, teacher_forcing_ratio=0.5):\n        # src: [batch_size, src_len]\n        # tgt: [batch_size, tgt_len]\n        \n        batch_size = src.shape[0]\n        tgt_len = tgt.shape[1]\n        tgt_vocab_size = self.decoder.vocab_size\n        \n        # Tensor to store decoder outputs\n        outputs = torch.zeros(batch_size, tgt_len, tgt_vocab_size).to(self.device)\n        \n        # Encode the source sequence\n        if self.encoder.cell_type == 'LSTM':\n            _, (hidden, cell) = self.encoder(src)\n            # hidden: [num_layers, batch_size, hidden_dim]\n            # cell: [num_layers, batch_size, hidden_dim]\n        else:  # RNN or GRU\n            _, hidden = self.encoder(src)\n            # hidden: [num_layers, batch_size, hidden_dim]\n        \n        # First input to the decoder is the <SOS> token\n        input = tgt[:, 0].unsqueeze(1)  # [batch_size, 1]\n        \n        for t in range(1, tgt_len):\n            # Pass through decoder\n            if self.decoder.cell_type == 'LSTM':\n                output, (hidden, cell) = self.decoder(input, (hidden, cell))\n                # output: [batch_size, vocab_size]\n                # hidden: [num_layers, batch_size, hidden_dim]\n                # cell: [num_layers, batch_size, hidden_dim]\n            else:  # RNN or GRU\n                output, hidden = self.decoder(input, hidden)\n                # output: [batch_size, vocab_size]\n                # hidden: [num_layers, batch_size, hidden_dim]\n            \n            # Store output\n            outputs[:, t, :] = output\n            \n            # Decide whether to use teacher forcing\n            teacher_force = random.random() < teacher_forcing_ratio\n            \n            # Get the highest predicted token\n            top1 = output.argmax(1)\n            \n            # Use ground truth or predicted token as next input\n            input = tgt[:, t].unsqueeze(1) if teacher_force else top1.unsqueeze(1)\n            \n        return outputs\n    \n    def translate(self, src, max_len=100):\n        # src: [batch_size, src_len]\n        \n        batch_size = src.shape[0]\n        \n        # Encode the source sequence\n        if self.encoder.cell_type == 'LSTM':\n            _, (hidden, cell) = self.encoder(src)\n        else:  # RNN or GRU\n            _, hidden = self.encoder(src)\n        \n        # First input to the decoder is the <SOS> token\n        input = torch.tensor([self.decoder.embedding.num_embeddings - 3] * batch_size).unsqueeze(1).to(self.device)  # SOS token\n        \n        outputs = []\n        \n        for t in range(max_len):\n            # Pass through decoder\n            if self.decoder.cell_type == 'LSTM':\n                output, (hidden, cell) = self.decoder(input, (hidden, cell))\n            else:  # RNN or GRU\n                output, hidden = self.decoder(input, hidden)\n            \n            # Get the highest predicted token\n            top1 = output.argmax(1)\n            \n            # Add to outputs\n            outputs.append(top1.unsqueeze(1))\n            \n            # Use predicted token as next input\n            input = top1.unsqueeze(1)\n            \n            # Break if EOS token is predicted (assuming EOS is 2)\n            if all(top1 == 2):\n                break\n                \n        # Concatenate all predictions\n        outputs = torch.cat(outputs, dim=1)\n        \n        return outputs\n\n# Initialize the model\ndef create_model(config, src_vocab_size, tgt_vocab_size, device):\n    encoder = Encoder(\n        vocab_size=src_vocab_size,\n        embedding_dim=config['emb_dim'],\n        hidden_dim=config['hidden_dim'],\n        num_layers=config['enc_layers'],\n        dropout=config['dropout'],\n        cell_type=config['cell_type']\n    )\n    \n    decoder = Decoder(\n        vocab_size=tgt_vocab_size,\n        embedding_dim=config['emb_dim'],\n        hidden_dim=config['hidden_dim'],\n        num_layers=config['dec_layers'],\n        dropout=config['dropout'],\n        cell_type=config['cell_type']\n    )\n    \n    model = Seq2Seq(encoder, decoder, device).to(device)\n    \n    return model\n\n# Calculate model parameters and computations\ndef calculate_model_stats(m, k, V, T):\n    \"\"\"\n    Calculate number of parameters and computations for a simple Seq2Seq model\n    \n    Args:\n        m: embedding size\n        k: hidden state size\n        V: vocabulary size\n        T: sequence length\n        \n    Returns:\n        num_params: number of parameters\n        num_comps: number of computations\n    \"\"\"\n    # Embedding parameters\n    emb_params = V * m\n    \n    # RNN cell parameters (assuming vanilla RNN)\n    # Input matrix: m x k, hidden matrix: k x k, bias: k\n    rnn_params = (m * k) + (k * k) + k\n    \n    # Output layer parameters\n    output_params = k * V + V\n    \n    # Total parameters for encoder and decoder\n    num_params = 2 * emb_params + 2 * rnn_params + output_params\n    \n    # Total computations\n    # For each time step:\n    # - embedding lookup: m\n    # - RNN computation: m*k + k*k\n    # - output layer: k*V\n    per_step_comp = m + (m * k) + (k * k) + (k * V)\n    num_comps = T * per_step_comp  # For encoder + decoder\n    \n    return num_params, num_comps\n\n# Example: m=64, k=128, V=100, T=10\nm, k, V, T = 64, 128, 100, 10\nnum_params, num_comps = calculate_model_stats(m, k, V, T)\nprint(f\"Number of parameters: {num_params}\")\nprint(f\"Number of computations: {num_comps}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T16:23:33.095250Z","iopub.execute_input":"2025-05-20T16:23:33.095917Z","iopub.status.idle":"2025-05-20T16:23:33.120190Z","shell.execute_reply.started":"2025-05-20T16:23:33.095894Z","shell.execute_reply":"2025-05-20T16:23:33.119602Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Initialize training and validation functions\ndef train_epoch(model, iterator, optimizer, criterion, teacher_forcing_ratio, clip, device):\n    model.train()\n    epoch_loss = 0\n    \n    for batch in tqdm(iterator, desc=\"Training\"):\n        src = batch[\"src\"].to(device)\n        tgt = batch[\"tgt\"].to(device)\n        \n        optimizer.zero_grad()\n        \n        # Forward pass\n        output = model(src, tgt, teacher_forcing_ratio)\n        \n        # Calculate loss (ignore pad tokens)\n        output_dim = output.shape[-1]\n        \n        # Reshape for cross entropy\n        output = output[:, 1:].reshape(-1, output_dim)\n        tgt = tgt[:, 1:].reshape(-1)\n        \n        loss = criterion(output, tgt)\n        \n        # Backward pass\n        loss.backward()\n        \n        # Clip gradients\n        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n        \n        # Update parameters\n        optimizer.step()\n        \n        epoch_loss += loss.item()\n    \n    return epoch_loss / len(iterator)\n\ndef evaluate(model, iterator, criterion, device):\n    model.eval()\n    epoch_loss = 0\n    correct_predictions = 0\n    total_samples = 0\n    all_predictions = []\n    all_targets = []\n    \n    with torch.no_grad():\n        for batch in tqdm(iterator, desc=\"Evaluating\"):\n            src = batch[\"src\"].to(device)\n            tgt = batch[\"tgt\"].to(device)\n            src_texts = batch[\"src_texts\"]\n            tgt_texts = batch[\"tgt_texts\"]\n            \n            # Forward pass (no teacher forcing)\n            output = model(src, tgt, 0)\n            \n            # Get predictions (translate mode)\n            predictions = model.translate(src)\n            \n            # Calculate loss (ignore pad tokens)\n            output_dim = output.shape[-1]\n            \n            # Reshape for cross entropy\n            output = output[:, 1:].reshape(-1, output_dim)\n            tgt = tgt[:, 1:].reshape(-1)\n            \n            loss = criterion(output, tgt)\n            \n            epoch_loss += loss.item()\n            \n            # Calculate accuracy\n            for i in range(len(src_texts)):\n                pred_seq = predictions[i].cpu().numpy()\n                # Remove padding, SOS, EOS\n                pred_seq = [idx for idx in pred_seq if idx not in [0, 1, 2]]\n                pred_text = ''.join([tgt_vocab.itos[idx] for idx in pred_seq])\n                \n                # Compare with target text (ignoring SOS, EOS)\n                target_text = tgt_texts[i]\n                \n                all_predictions.append(pred_text)\n                all_targets.append(target_text)\n                \n                # Check if prediction matches target\n                if pred_text == target_text:\n                    correct_predictions += 1\n                \n                total_samples += 1\n    \n    accuracy = correct_predictions / total_samples if total_samples > 0 else 0\n    \n    return epoch_loss / len(iterator), accuracy, all_predictions, all_targets\n\n# Training loop\ndef train_model(config, model, train_loader, val_loader, criterion, optimizer, scheduler, device, run):\n    # Set parameters\n    n_epochs = config[\"epochs\"]\n    clip = 1.0\n    teacher_forcing_ratio = config[\"teacher_forcing\"]\n    patience = config[\"patience\"]\n    \n    # Initialize for early stopping\n    best_valid_loss = float('inf')\n    counter = 0\n    best_model = None\n    \n    for epoch in range(n_epochs):\n        # Train\n        train_loss = train_epoch(model, train_loader, optimizer, criterion, teacher_forcing_ratio, clip, device)\n        \n        # Evaluate\n        valid_loss, valid_accuracy, _, _ = evaluate(model, val_loader, criterion, device)\n        \n        # Update learning rate\n        scheduler.step(valid_loss)\n        \n        # Log to W&B\n        run.log({\n            \"train_loss\": train_loss,\n            \"val_loss\": valid_loss,\n            \"val_sequence_accuracy\": valid_accuracy,\n            \"epoch\": epoch + 1\n        })\n        \n        print(f\"Epoch: {epoch+1}\")\n        print(f\"Train Loss: {train_loss:.4f}\")\n        print(f\"Validation Loss: {valid_loss:.4f}\")\n        print(f\"Validation Accuracy: {valid_accuracy:.4f}\")\n        \n        # Early stopping\n        if valid_loss < best_valid_loss:\n            best_valid_loss = valid_loss\n            best_model = model.state_dict().copy()\n            counter = 0\n        else:\n            counter += 1\n            if counter >= patience:\n                print(f\"Early stopping after {epoch+1} epochs\")\n                break\n    \n    # Load best model\n    model.load_state_dict(best_model)\n    \n    return model\n\n# Function to test the model and get sample predictions\ndef test_model(model, test_loader, device, tgt_vocab):\n    model.eval()\n    all_predictions = []\n    all_targets = []\n    all_sources = []\n    \n    correct_predictions = 0\n    total_samples = 0\n    \n    with torch.no_grad():\n        for batch in tqdm(test_loader, desc=\"Testing\"):\n            src = batch[\"src\"].to(device)\n            tgt = batch[\"tgt\"].to(device)\n            src_texts = batch[\"src_texts\"]\n            tgt_texts = batch[\"tgt_texts\"]\n            \n            # Get predictions\n            predictions = model.translate(src)\n            \n            # Process predictions\n            for i in range(len(src_texts)):\n                pred_seq = predictions[i].cpu().numpy()\n                # Remove padding, SOS, EOS\n                pred_seq = [idx for idx in pred_seq if idx not in [0, 1, 2]]\n                pred_text = ''.join([tgt_vocab.itos[idx] for idx in pred_seq])\n                \n                # Store results\n                all_sources.append(src_texts[i])\n                all_predictions.append(pred_text)\n                all_targets.append(tgt_texts[i])\n                \n                # Check if prediction matches target\n                if pred_text == tgt_texts[i]:\n                    correct_predictions += 1\n                \n                total_samples += 1\n    \n    # Calculate accuracy\n    accuracy = correct_predictions / total_samples if total_samples > 0 else 0\n    \n    # Sample some predictions for display\n    num_samples = min(10, len(all_sources))\n    sample_indices = np.random.choice(len(all_sources), num_samples, replace=False)\n    \n    sample_data = {\n        \"Source\": [all_sources[i] for i in sample_indices],\n        \"Target\": [all_targets[i] for i in sample_indices],\n        \"Prediction\": [all_predictions[i] for i in sample_indices],\n        \"Correct\": [all_predictions[i] == all_targets[i] for i in sample_indices]\n    }\n    \n    return accuracy, all_sources, all_targets, all_predictions, sample_data\n\n# Save predictions to file\ndef save_predictions(sources, targets, predictions, filename):\n    df = pd.DataFrame({\n        \"Source\": sources,\n        \"Target\": targets,\n        \"Prediction\": predictions,\n        \"Correct\": [p == t for p, t in zip(predictions, targets)]\n    })\n    df.to_csv(filename, index=False)\n    return df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T16:26:09.494474Z","iopub.execute_input":"2025-05-20T16:26:09.495009Z","iopub.status.idle":"2025-05-20T16:26:09.512915Z","shell.execute_reply.started":"2025-05-20T16:26:09.494965Z","shell.execute_reply":"2025-05-20T16:26:09.512344Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Initialize W&B sweep\ndef train_with_config():\n    # Login to wandb\n    wandb.login()\n    \n    # Define sweep config\n    sweep_config = {\n        'method': 'bayes',\n        'metric': {'name': 'val_sequence_accuracy', 'goal': 'maximize'},\n        'parameters': {\n            'emb_dim': {'values': [64, 128, 256]},\n            'hidden_dim': {'values': [128, 256]},\n            'enc_layers': {'values': [1, 2, 3]},\n            'dec_layers': {'values': [1, 2, 3]},\n            'cell_type': {'values': ['LSTM', 'GRU', 'RNN']},\n            'dropout': {'values': [0.2, 0.3, 0.4]},\n            'batch_size': {'values': [32, 64, 128]},\n            'learning_rate': {'values': [0.001, 0.0005, 0.0001]},\n            'teacher_forcing': {'values': [0.5, 0.7, 0.9]},\n            'patience': {'value': 3},\n            'epochs': {'values': [10, 15]}\n        }\n    }\n    \n    # Initialize sweep\n    sweep_id = wandb.sweep(sweep_config, project=\"transliteration-seq2seq\")\n    \n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T16:26:09.514079Z","iopub.execute_input":"2025-05-20T16:26:09.514371Z","iopub.status.idle":"2025-05-20T16:26:09.528587Z","shell.execute_reply.started":"2025-05-20T16:26:09.514354Z","shell.execute_reply":"2025-05-20T16:26:09.528021Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Initialize W&B sweep\ndef train_with_config():\n    # Login to wandb\n    wandb.login()\n    \n    # Define sweep config\n    sweep_config = {\n        'method': 'bayes',\n        'metric': {'name': 'val_sequence_accuracy', 'goal': 'maximize'},\n        'parameters': {\n            'emb_dim': {'values': [64, 128, 256]},\n            'hidden_dim': {'values': [128, 256]},\n            'enc_layers': {'values': [1, 2, 3]},\n            'dec_layers': {'values': [1, 2, 3]},\n            'cell_type': {'values': ['LSTM', 'GRU', 'RNN']},\n            'dropout': {'values': [0.2, 0.3, 0.4]},\n            'batch_size': {'values': [32, 64, 128]},\n            'learning_rate': {'values': [0.001, 0.0005, 0.0001]},\n            'teacher_forcing': {'values': [0.5, 0.7, 0.9]},\n            'patience': {'value': 3},\n            'epochs': {'values': [10, 15]}\n        }\n    }\n    \n    # Initialize sweep\n    sweep_id = wandb.sweep(sweep_config, project=\"transliteration-seq2seq\")\n    \n    # Define the training function for each sweep run\n    def train_sweep():\n        # Initialize wandb run\n        run = wandb.init()\n        \n        # Get hyperparameters for this run\n        config = wandb.config\n        \n        # Create dataloaders\n        train_loader = DataLoader(\n            train_dataset,\n            batch_size=config.batch_size,\n            shuffle=True,\n            collate_fn=pad_collate\n        )\n        \n        val_loader = DataLoader(\n            val_dataset,\n            batch_size=config.batch_size,\n            shuffle=False,\n            collate_fn=pad_collate\n        )\n        \n        # Create model\n        model = create_model(\n            config,\n            len(src_vocab),\n            len(tgt_vocab),\n            device\n        )\n        \n        # Define loss function and optimizer\n        criterion = nn.CrossEntropyLoss(ignore_index=0)  # Ignore padding tokens\n        optimizer = optim.Adam(model.parameters(), lr=config.learning_rate)\n        scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n            optimizer, mode='min', factor=0.5, patience=1, verbose=True\n        )\n        \n        # Train the model\n        model = train_model(\n            config,\n            model,\n            train_loader,\n            val_loader,\n            criterion,\n            optimizer,\n            scheduler,\n            device,\n            run\n        )\n        \n        # Test on validation set for final metrics\n        valid_loss, valid_accuracy, valid_preds, valid_targets = evaluate(\n            model, val_loader, criterion, device\n        )\n        \n        # Log final metrics\n        run.log({\n            \"final_val_loss\": valid_loss,\n            \"final_val_accuracy\": valid_accuracy\n        })\n        \n        # Save the model\n        torch.save(model.state_dict(), f\"model_{run.id}.pt\")\n        \n        # Finish the run\n        run.finish()\n    \n    # Run the sweep\n    wandb.agent(sweep_id, train_sweep, count=10)  # Run 10 experiments\n    \n    return sweep_id","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T16:26:26.980962Z","iopub.execute_input":"2025-05-20T16:26:26.981742Z","iopub.status.idle":"2025-05-20T16:26:26.990847Z","shell.execute_reply.started":"2025-05-20T16:26:26.981717Z","shell.execute_reply":"2025-05-20T16:26:26.990132Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Main execution script\ndef main():\n    # Run sweep to find best hyperparameters\n    print(\"Starting hyperparameter sweep...\")\n    sweep_id = train_with_config()\n    \n    # Get the best run from the sweep\n    api = wandb.Api()\n    sweep = api.sweep(f\"da24m002-indian-institute-of-technology-madras/transliteration-seq2seq/{sweep_id}\")\n    \n    # Sort runs by validation accuracy\n    runs = sorted(sweep.runs, key=lambda run: run.summary.get('val_sequence_accuracy', 0), reverse=True)\n    \n    if runs:\n        best_run = runs[0]\n        best_config = {\n            k: v for k, v in best_run.config.items() \n            if k in ['emb_dim', 'hidden_dim', 'enc_layers', 'dec_layers', \n                    'cell_type', 'dropout', 'batch_size', 'learning_rate', \n                    'teacher_forcing', 'patience', 'epochs']\n        }\n        \n        print(f\"Best run: {best_run.name}\")\n        print(f\"Best validation accuracy: {best_run.summary.get('val_sequence_accuracy', 0)}\")\n        print(f\"Best configuration: {best_config}\")\n        \n        # Create test dataloader\n        test_loader = DataLoader(\n            test_dataset,\n            batch_size=best_config['batch_size'],\n            shuffle=False,\n            collate_fn=pad_collate\n        )\n        \n        # Create model with best config\n        best_model = create_model(\n            best_config,\n            len(src_vocab),\n            len(tgt_vocab),\n            device\n        )\n        \n        # Load best model weights\n        best_model.load_state_dict(torch.load(f\"model_{best_run.id}.pt\"))\n        \n        # Evaluate on test set\n        print(\"Evaluating best model on test set...\")\n        test_accuracy, test_sources, test_targets, test_preds, sample_data = test_model(\n            best_model, test_loader, device, tgt_vocab\n        )\n        \n        print(f\"Test accuracy: {test_accuracy:.4f}\")\n        \n        # Create directory for predictions if it doesn't exist\n        if not os.path.exists('predictions_vanilla'):\n            os.makedirs('predictions_vanilla')\n        \n        # Save predictions\n        save_predictions(\n            test_sources,\n            test_targets,\n            test_preds,\n            'predictions_vanilla/test_predictions.csv'\n        )\n        \n        # Print sample predictions\n        print(\"\\nSample predictions:\")\n        for i in range(len(sample_data['Source'])):\n            print(f\"Source: {sample_data['Source'][i]}\")\n            print(f\"Target: {sample_data['Target'][i]}\")\n            print(f\"Prediction: {sample_data['Prediction'][i]}\")\n            print(f\"Correct: {'✓' if sample_data['Correct'][i] else '✗'}\")\n            print(\"---\")\n    else:\n        print(\"No runs found in sweep.\")\n\n# Run the code for question 4 directly (testing on test data)\ndef run_question4(best_config=None):\n    \"\"\"\n    Run evaluation on the test set directly using a predefined configuration\n    This can be used if you want to skip the sweep\n    \"\"\"\n    if best_config is None:\n        # Define a default configuration if none provided\n        best_config = {\n            'emb_dim': 128,\n            'hidden_dim': 256,\n            'enc_layers': 3,\n            'dec_layers': 3,\n            'cell_type': 'GRU',\n            'dropout': 0.4,\n            'batch_size': 64,\n            'learning_rate': 0.001,\n            'teacher_forcing': 0.7,\n            'patience': 3,\n            'epochs': 15\n        }\n    \n    # Initialize wandb\n    wandb.init(project=\"transliteration-direct-test\", config=best_config)\n    \n    # Create data loaders\n    train_loader = DataLoader(\n        train_dataset,\n        batch_size=best_config['batch_size'],\n        shuffle=True,\n        collate_fn=pad_collate\n    )\n    \n    val_loader = DataLoader(\n        val_dataset,\n        batch_size=best_config['batch_size'],\n        shuffle=False,\n        collate_fn=pad_collate\n    )\n    \n    test_loader = DataLoader(\n        test_dataset,\n        batch_size=best_config['batch_size'],\n        shuffle=False,\n        collate_fn=pad_collate\n    )\n    \n    # Create model\n    model = create_model(\n        best_config,\n        len(src_vocab),\n        len(tgt_vocab),\n        device\n    )\n    \n    # Define loss function and optimizer\n    criterion = nn.CrossEntropyLoss(ignore_index=0)  # Ignore padding tokens\n    optimizer = optim.Adam(model.parameters(), lr=best_config['learning_rate'])\n    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n        optimizer, mode='min', factor=0.5, patience=1, verbose=True\n    )\n    \n    # Train the model\n    print(\"Training model with best configuration...\")\n    model = train_model(\n        best_config,\n        model,\n        train_loader,\n        val_loader,\n        criterion,\n        optimizer,\n        scheduler,\n        device,\n        wandb.run\n    )\n    \n    # Save the trained model\n    torch.save(model.state_dict(), f\"best_model.pt\")\n    \n    # Evaluate on test set\n    print(\"Evaluating model on test set...\")\n    test_accuracy, test_sources, test_targets, test_preds, sample_data = test_model(\n        model, test_loader, device, tgt_vocab\n    )\n    \n    print(f\"Test accuracy: {test_accuracy:.4f}\")\n    \n    # Create directory for predictions if it doesn't exist\n    if not os.path.exists('predictions_vanilla'):\n        os.makedirs('predictions_vanilla')\n    \n    # Save predictions\n    predictions_df = save_predictions(\n        test_sources,\n        test_targets,\n        test_preds,\n        'predictions_vanilla/test_predictions.csv'\n    )\n    \n    # Log predictions table to wandb\n    wandb.log({\n        \"test_accuracy\": test_accuracy,\n        \"predictions_table\": wandb.Table(dataframe=predictions_df.sample(min(100, len(predictions_df))))\n    })\n    \n    # Print sample predictions\n    print(\"\\nSample predictions:\")\n    sample_indices = np.random.choice(len(predictions_df), min(10, len(predictions_df)), replace=False)\n    for idx in sample_indices:\n        print(f\"Source: {predictions_df.iloc[idx]['Source']}\")\n        print(f\"Target: {predictions_df.iloc[idx]['Target']}\")\n        print(f\"Prediction: {predictions_df.iloc[idx]['Prediction']}\")\n        print(f\"Correct: {'✓' if predictions_df.iloc[idx]['Correct'] else '✗'}\")\n        print(\"---\")\n    \n    # Analyze errors\n    error_indices = predictions_df[~predictions_df['Correct']].index\n    error_df = predictions_df.loc[error_indices]\n    \n    print(f\"\\nTotal errors: {len(error_df)}\")\n    print(f\"Error rate: {len(error_df) / len(predictions_df):.4f}\")\n    \n    # Simple error analysis\n    if len(error_df) > 0:\n        error_lengths = error_df['Source'].str.len()\n        correct_lengths = predictions_df[predictions_df['Correct']]['Source'].str.len()\n        \n        print(f\"Average length of error cases: {error_lengths.mean():.2f}\")\n        print(f\"Average length of correct cases: {correct_lengths.mean():.2f}\")\n        \n        # Find most common error patterns\n        error_pairs = list(zip(error_df['Target'], error_df['Prediction']))\n        common_errors = Counter(error_pairs).most_common(5)\n        \n        print(\"\\nMost common errors (Target -> Prediction):\")\n        for (target, pred), count in common_errors:\n            print(f\"  '{target}' -> '{pred}': {count} occurrences\")\n    \n    wandb.finish()\n    \n    return model, test_accuracy, predictions_df\n\n# You can choose to run either the full sweep or just Question 4 directly\nif __name__ == \"__main__\":\n    # Choose one of the following:\n    \n    # Option 1: Run the full sweep and evaluation\n    # main()\n    \n    # Option 2: Run Question 4 directly with a predefined config\n    run_question4()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T16:26:31.877026Z","iopub.execute_input":"2025-05-20T16:26:31.877571Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}