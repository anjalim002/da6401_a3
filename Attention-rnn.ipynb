{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "editable": false,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Cell 1: Import libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import wandb\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "\n",
    "import wandb\n",
    "wandb.login(key=\"5377a200ae8c04c015415319969d0f2ea19c027c\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Cell 2: Dataset class and data loading\n",
    "class TransliterationDataset(Dataset):\n",
    "    def __init__(self, latin_texts, devanagari_texts, latin_vocab, devanagari_vocab):\n",
    "        self.latin_texts = latin_texts\n",
    "        self.devanagari_texts = devanagari_texts\n",
    "        self.latin_vocab = latin_vocab\n",
    "        self.devanagari_vocab = devanagari_vocab\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.latin_texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        latin_text = self.latin_texts[idx]\n",
    "        devanagari_text = self.devanagari_texts[idx]\n",
    "        \n",
    "        # Convert to indices\n",
    "        latin_indices = [self.latin_vocab.get(char, self.latin_vocab['<UNK>']) for char in latin_text]\n",
    "        devanagari_indices = [self.devanagari_vocab.get(char, self.devanagari_vocab['<UNK>']) for char in devanagari_text]\n",
    "        \n",
    "        # Add EOS token\n",
    "        latin_indices.append(self.latin_vocab['<EOS>'])\n",
    "        devanagari_indices.append(self.devanagari_vocab['<EOS>'])\n",
    "        \n",
    "        return {\n",
    "            'latin': torch.tensor(latin_indices, dtype=torch.long),\n",
    "            'devanagari': torch.tensor(devanagari_indices, dtype=torch.long),\n",
    "            'latin_len': len(latin_indices),\n",
    "            'devanagari_len': len(devanagari_indices),\n",
    "            'latin_text': latin_text,\n",
    "            'devanagari_text': devanagari_text\n",
    "        }\n",
    "\n",
    "def collate_fn(batch):\n",
    "    latin_lens = [item['latin_len'] for item in batch]\n",
    "    devanagari_lens = [item['devanagari_len'] for item in batch]\n",
    "    \n",
    "    max_latin_len = max(latin_lens)\n",
    "    max_devanagari_len = max(devanagari_lens)\n",
    "    \n",
    "    latin_padded = torch.zeros((len(batch), max_latin_len), dtype=torch.long)\n",
    "    devanagari_padded = torch.zeros((len(batch), max_devanagari_len), dtype=torch.long)\n",
    "    \n",
    "    latin_texts = []\n",
    "    devanagari_texts = []\n",
    "    \n",
    "    for i, item in enumerate(batch):\n",
    "        latin = item['latin']\n",
    "        devanagari = item['devanagari']\n",
    "        \n",
    "        latin_padded[i, :latin.size(0)] = latin\n",
    "        devanagari_padded[i, :devanagari.size(0)] = devanagari\n",
    "        \n",
    "        latin_texts.append(item['latin_text'])\n",
    "        devanagari_texts.append(item['devanagari_text'])\n",
    "    \n",
    "    return {\n",
    "        'latin': latin_padded,\n",
    "        'devanagari': devanagari_padded,\n",
    "        'latin_lens': torch.tensor(latin_lens),\n",
    "        'devanagari_lens': torch.tensor(devanagari_lens),\n",
    "        'latin_texts': latin_texts,\n",
    "        'devanagari_texts': devanagari_texts\n",
    "    }\n",
    "def load_data(train_path, dev_path, test_path):\n",
    "    # Load only the first two relevant columns: 0 (output), 1 (input)\n",
    "    train_df = pd.read_csv(train_path, sep='\\t', header=None, usecols=[0, 1])\n",
    "    dev_df = pd.read_csv(dev_path, sep='\\t', header=None, usecols=[0, 1])\n",
    "    test_df = pd.read_csv(test_path, sep='\\t', header=None, usecols=[0, 1])\n",
    "\n",
    "    train_df.dropna(inplace=True)\n",
    "    dev_df.dropna(inplace=True)\n",
    "    test_df.dropna(inplace=True)\n",
    "    \n",
    "    # Extract input (latin) and output (devanagari) texts\n",
    "    train_devanagari, train_latin = train_df[0].tolist(), train_df[1].tolist()\n",
    "    dev_devanagari, dev_latin = dev_df[0].tolist(), dev_df[1].tolist()\n",
    "    test_devanagari, test_latin = test_df[0].tolist(), test_df[1].tolist()\n",
    "    \n",
    "    # Build vocabularies\n",
    "    latin_chars = set()\n",
    "    devanagari_chars = set()\n",
    "    \n",
    "    for text in train_latin + dev_latin + test_latin:\n",
    "        latin_chars.update(str(text))  # handle any NaN\n",
    "    \n",
    "    for text in train_devanagari + dev_devanagari + test_devanagari:\n",
    "        devanagari_chars.update(str(text))  # handle any NaN\n",
    "    \n",
    "    # Create vocabularies with special tokens\n",
    "    latin_vocab = {'<PAD>': 0, '<UNK>': 1, '<SOS>': 2, '<EOS>': 3}\n",
    "    for i, char in enumerate(sorted(latin_chars)):\n",
    "        latin_vocab[char] = i + 4\n",
    "    \n",
    "    devanagari_vocab = {'<PAD>': 0, '<UNK>': 1, '<SOS>': 2, '<EOS>': 3}\n",
    "    for i, char in enumerate(sorted(devanagari_chars)):\n",
    "        devanagari_vocab[char] = i + 4\n",
    "    \n",
    "    # Create inverse vocabularies for decoding\n",
    "    latin_inv_vocab = {v: k for k, v in latin_vocab.items()}\n",
    "    devanagari_inv_vocab = {v: k for k, v in devanagari_vocab.items()}\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = TransliterationDataset(train_latin, train_devanagari, latin_vocab, devanagari_vocab)\n",
    "    dev_dataset = TransliterationDataset(dev_latin, dev_devanagari, latin_vocab, devanagari_vocab)\n",
    "    test_dataset = TransliterationDataset(test_latin, test_devanagari, latin_vocab, devanagari_vocab)\n",
    "    \n",
    "    return {\n",
    "        'train_dataset': train_dataset,\n",
    "        'dev_dataset': dev_dataset,\n",
    "        'test_dataset': test_dataset,\n",
    "        'latin_vocab': latin_vocab,\n",
    "        'devanagari_vocab': devanagari_vocab,\n",
    "        'latin_inv_vocab': latin_inv_vocab,\n",
    "        'devanagari_inv_vocab': devanagari_inv_vocab\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Cell 3: Attention-based Seq2Seq model\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, hidden_dim, n_layers, dropout, cell_type):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.emb_dim = emb_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout = dropout\n",
    "        self.cell_type = cell_type\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
    "        self.dropout_layer = nn.Dropout(dropout)\n",
    "        \n",
    "        if cell_type == 'RNN':\n",
    "            self.rnn = nn.RNN(emb_dim, hidden_dim, n_layers, dropout=dropout if n_layers > 1 else 0, batch_first=True)\n",
    "        elif cell_type == 'LSTM':\n",
    "            self.rnn = nn.LSTM(emb_dim, hidden_dim, n_layers, dropout=dropout if n_layers > 1 else 0, batch_first=True)\n",
    "        elif cell_type == 'GRU':\n",
    "            self.rnn = nn.GRU(emb_dim, hidden_dim, n_layers, dropout=dropout if n_layers > 1 else 0, batch_first=True)\n",
    "        \n",
    "    def forward(self, src, src_lens):\n",
    "        # src: [batch_size, src_len]\n",
    "        \n",
    "        embedded = self.dropout_layer(self.embedding(src))\n",
    "        # embedded: [batch_size, src_len, emb_dim]\n",
    "        \n",
    "        # Pack padded sequence\n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, src_lens.cpu(), batch_first=True, enforce_sorted=False)\n",
    "        \n",
    "        if self.cell_type == 'LSTM':\n",
    "            packed_outputs, (hidden, cell) = self.rnn(packed_embedded)\n",
    "            outputs, _ = nn.utils.rnn.pad_packed_sequence(packed_outputs, batch_first=True)\n",
    "            return outputs, hidden, cell\n",
    "        else:\n",
    "            packed_outputs, hidden = self.rnn(packed_embedded)\n",
    "            outputs, _ = nn.utils.rnn.pad_packed_sequence(packed_outputs, batch_first=True)\n",
    "            return outputs, hidden\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        self.attn = nn.Linear(hidden_dim * 2, hidden_dim)\n",
    "        self.v = nn.Linear(hidden_dim, 1, bias=False)\n",
    "        \n",
    "    def forward(self, hidden, encoder_outputs, mask=None):\n",
    "        # hidden: [batch_size, hidden_dim]\n",
    "        # encoder_outputs: [batch_size, src_len, hidden_dim]\n",
    "        \n",
    "        batch_size = encoder_outputs.shape[0]\n",
    "        src_len = encoder_outputs.shape[1]\n",
    "        \n",
    "        # Repeat hidden for each word in the source\n",
    "        hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)\n",
    "        # hidden: [batch_size, src_len, hidden_dim]\n",
    "        \n",
    "        # Concatenate encoder outputs and hidden state\n",
    "        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim=2)))\n",
    "        # energy: [batch_size, src_len, hidden_dim]\n",
    "        \n",
    "        attention = self.v(energy).squeeze(2)\n",
    "        # attention: [batch_size, src_len]\n",
    "        \n",
    "        if mask is not None:\n",
    "            attention = attention.masked_fill(mask == 0, -1e10)\n",
    "        \n",
    "        return torch.softmax(attention, dim=1)\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, hidden_dim, n_layers, dropout, cell_type, attention):\n",
    "        super().__init__()\n",
    "        self.output_dim = output_dim\n",
    "        self.emb_dim = emb_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout = dropout\n",
    "        self.cell_type = cell_type\n",
    "        self.attention = attention\n",
    "        \n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
    "        self.dropout_layer = nn.Dropout(dropout)\n",
    "        \n",
    "        if cell_type == 'RNN':\n",
    "            self.rnn = nn.RNN(emb_dim + hidden_dim, hidden_dim, n_layers, dropout=dropout if n_layers > 1 else 0, batch_first=True)\n",
    "        elif cell_type == 'LSTM':\n",
    "            self.rnn = nn.LSTM(emb_dim + hidden_dim, hidden_dim, n_layers, dropout=dropout if n_layers > 1 else 0, batch_first=True)\n",
    "        elif cell_type == 'GRU':\n",
    "            self.rnn = nn.GRU(emb_dim + hidden_dim, hidden_dim, n_layers, dropout=dropout if n_layers > 1 else 0, batch_first=True)\n",
    "        \n",
    "        self.fc_out = nn.Linear(hidden_dim * 2 + emb_dim, output_dim)\n",
    "        \n",
    "    def forward(self, input, hidden, encoder_outputs, cell=None, mask=None):\n",
    "        # input: [batch_size, 1]\n",
    "        # hidden: [n_layers, batch_size, hidden_dim]\n",
    "        # encoder_outputs: [batch_size, src_len, hidden_dim]\n",
    "        \n",
    "        input = input.unsqueeze(1)  # [batch_size, 1]\n",
    "        \n",
    "        embedded = self.dropout_layer(self.embedding(input))\n",
    "        # embedded: [batch_size, 1, emb_dim]\n",
    "        \n",
    "        # Get attention weights\n",
    "        if self.cell_type == 'LSTM':\n",
    "            # For LSTM, use the top layer hidden state for attention\n",
    "            attn_weights = self.attention(hidden[-1], encoder_outputs, mask)\n",
    "        else:\n",
    "            attn_weights = self.attention(hidden[-1], encoder_outputs, mask)\n",
    "        \n",
    "        # attn_weights: [batch_size, src_len]\n",
    "        \n",
    "        # Create context vector by multiplying attention weights with encoder outputs\n",
    "        attn_weights = attn_weights.unsqueeze(1)\n",
    "        # attn_weights: [batch_size, 1, src_len]\n",
    "        \n",
    "        context = torch.bmm(attn_weights, encoder_outputs)\n",
    "        # context: [batch_size, 1, hidden_dim]\n",
    "        \n",
    "        # Combine embedded input and context vector\n",
    "        rnn_input = torch.cat((embedded, context), dim=2)\n",
    "        # rnn_input: [batch_size, 1, emb_dim + hidden_dim]\n",
    "        \n",
    "        if self.cell_type == 'LSTM':\n",
    "            output, (hidden, cell) = self.rnn(rnn_input, (hidden, cell))\n",
    "            # output: [batch_size, 1, hidden_dim]\n",
    "        else:\n",
    "            output, hidden = self.rnn(rnn_input, hidden)\n",
    "            # output: [batch_size, 1, hidden_dim]\n",
    "        \n",
    "        # Squeeze the sequence length dimension\n",
    "        output = output.squeeze(1)\n",
    "        context = context.squeeze(1)\n",
    "        embedded = embedded.squeeze(1)\n",
    "        \n",
    "        # Make prediction\n",
    "        prediction = self.fc_out(torch.cat((output, context, embedded), dim=1))\n",
    "        # prediction: [batch_size, output_dim]\n",
    "        \n",
    "        if self.cell_type == 'LSTM':\n",
    "            return prediction, hidden, cell, attn_weights\n",
    "        else:\n",
    "            return prediction, hidden, attn_weights\n",
    "\n",
    "class Seq2SeqWithAttention(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "        \n",
    "        assert encoder.hidden_dim == decoder.hidden_dim, \"Hidden dimensions of encoder and decoder must be equal!\"\n",
    "        \n",
    "    def create_mask(self, src, src_lens):\n",
    "        # src: [batch_size, src_len]\n",
    "        batch_size = src.shape[0]\n",
    "        max_src_len = src.shape[1]\n",
    "        \n",
    "        # Create a mask of shape [batch_size, src_len]\n",
    "        mask = torch.zeros(batch_size, max_src_len, device=self.device)\n",
    "        \n",
    "        for i, length in enumerate(src_lens):\n",
    "            mask[i, :length] = 1\n",
    "            \n",
    "        return mask\n",
    "    \n",
    "    def forward(self, src, src_lens, trg, teacher_forcing_ratio=0.5):\n",
    "        # src: [batch_size, src_len]\n",
    "        # trg: [batch_size, trg_len]\n",
    "        \n",
    "        batch_size = src.shape[0]\n",
    "        trg_len = trg.shape[1]\n",
    "        trg_vocab_size = self.decoder.output_dim\n",
    "        \n",
    "        # Tensor to store decoder outputs\n",
    "        outputs = torch.zeros(batch_size, trg_len, trg_vocab_size).to(self.device)\n",
    "        \n",
    "        # Tensor to store attention weights\n",
    "        attentions = torch.zeros(batch_size, trg_len, src.shape[1]).to(self.device)\n",
    "        \n",
    "        # Create a mask for encoder outputs\n",
    "        mask = self.create_mask(src, src_lens)\n",
    "        \n",
    "        # Encode the source sequence\n",
    "        if self.encoder.cell_type == 'LSTM':\n",
    "            encoder_outputs, hidden, cell = self.encoder(src, src_lens)\n",
    "        else:\n",
    "            encoder_outputs, hidden = self.encoder(src, src_lens)\n",
    "            cell = None\n",
    "        \n",
    "        # First input to the decoder is the <SOS> token\n",
    "        input = trg[:, 0]\n",
    "        \n",
    "        for t in range(1, trg_len):\n",
    "            # Decode one step\n",
    "            if self.decoder.cell_type == 'LSTM':\n",
    "                output, hidden, cell, attn_weights = self.decoder(input, hidden, encoder_outputs, cell, mask)\n",
    "            else:\n",
    "                output, hidden, attn_weights = self.decoder(input, hidden, encoder_outputs, mask=mask)\n",
    "            \n",
    "            # Store output and attention\n",
    "            outputs[:, t, :] = output\n",
    "            attentions[:, t, :] = attn_weights.squeeze(1)\n",
    "            \n",
    "            # Teacher forcing: use real target as next input or use predicted token\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            top1 = output.argmax(1)\n",
    "            \n",
    "            input = trg[:, t] if teacher_force else top1\n",
    "            \n",
    "        return outputs, attentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Cell 4: Training, evaluation, and testing functions\n",
    "def train(model, iterator, optimizer, criterion, teacher_forcing_ratio, clip, device):\n",
    "    model.train()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for batch in iterator:\n",
    "        src = batch['latin'].to(device)\n",
    "        trg = batch['devanagari'].to(device)\n",
    "        src_lens = batch['latin_lens']\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output, _ = model(src, src_lens, trg, teacher_forcing_ratio)\n",
    "        \n",
    "        # Exclude the first token (SOS) from the loss calculation\n",
    "        output_dim = output.shape[-1]\n",
    "        output = output[:, 1:].reshape(-1, output_dim)\n",
    "        trg = trg[:, 1:].reshape(-1)\n",
    "        \n",
    "        loss = criterion(output, trg)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        # Clip gradients to prevent exploding gradients\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "    return epoch_loss / len(iterator)\n",
    "\n",
    "def evaluate(model, iterator, criterion, device):\n",
    "    model.eval()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            src = batch['latin'].to(device)\n",
    "            trg = batch['devanagari'].to(device)\n",
    "            src_lens = batch['latin_lens']\n",
    "            \n",
    "            output, _ = model(src, src_lens, trg, 0)  # No teacher forcing during evaluation\n",
    "            \n",
    "            # Exclude the first token (SOS) from the loss calculation\n",
    "            output_dim = output.shape[-1]\n",
    "            output = output[:, 1:].reshape(-1, output_dim)\n",
    "            trg = trg[:, 1:].reshape(-1)\n",
    "            \n",
    "            loss = criterion(output, trg)\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "            # Calculate sequence accuracy\n",
    "            output = output.view(batch['devanagari'].shape[0], -1, output_dim)\n",
    "            predictions = output.argmax(2)\n",
    "            \n",
    "            # Compare predictions with targets (excluding padding)\n",
    "            for i in range(len(batch['devanagari_texts'])):\n",
    "                pred_seq = []\n",
    "                for j in range(1, batch['devanagari_lens'][i] - 1):  # Exclude SOS and EOS\n",
    "                    pred_token = predictions[i, j-1].item()\n",
    "                    if pred_token != 0:  # Not PAD\n",
    "                        pred_seq.append(pred_token)\n",
    "                \n",
    "                true_seq = []\n",
    "                for j in range(1, batch['devanagari_lens'][i] - 1):  # Exclude SOS and EOS\n",
    "                    true_token = trg[i * (batch['devanagari_lens'].max().item() - 1) + (j-1)].item()\n",
    "                    if true_token != 0:  # Not PAD\n",
    "                        true_seq.append(true_token)\n",
    "                \n",
    "                if len(pred_seq) == len(true_seq) and all(p == t for p, t in zip(pred_seq, true_seq)):\n",
    "                    correct_predictions += 1\n",
    "                    \n",
    "                total_predictions += 1\n",
    "    \n",
    "    accuracy = correct_predictions / total_predictions if total_predictions > 0 else 0\n",
    "    \n",
    "    return epoch_loss / len(iterator), accuracy\n",
    "\n",
    "def translate_sentence(sentence, src_vocab, trg_vocab, model, device, max_len=100):\n",
    "    model.eval()\n",
    "    \n",
    "    # Convert sentence to indices\n",
    "    tokens = [src_vocab.get(char, src_vocab['<UNK>']) for char in sentence]\n",
    "    tokens.append(src_vocab['<EOS>'])\n",
    "    \n",
    "    src_tensor = torch.LongTensor(tokens).unsqueeze(0).to(device)\n",
    "    src_lens = torch.LongTensor([len(tokens)])\n",
    "    \n",
    "    # Create mask for encoder outputs\n",
    "    mask = model.create_mask(src_tensor, src_lens)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        if model.encoder.cell_type == 'LSTM':\n",
    "            encoder_outputs, hidden, cell = model.encoder(src_tensor, src_lens)\n",
    "        else:\n",
    "            encoder_outputs, hidden = model.encoder(src_tensor, src_lens)\n",
    "            cell = None\n",
    "    \n",
    "    # First token is SOS\n",
    "    trg_idx = [trg_vocab['<SOS>']]\n",
    "    attentions = []\n",
    "    \n",
    "    for i in range(max_len):\n",
    "        trg_tensor = torch.LongTensor([trg_idx[-1]]).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            if model.decoder.cell_type == 'LSTM':\n",
    "                output, hidden, cell, attention = model.decoder(trg_tensor, hidden, encoder_outputs, cell, mask)\n",
    "            else:\n",
    "                output, hidden, attention = model.decoder(trg_tensor, hidden, encoder_outputs, mask=mask)\n",
    "                \n",
    "        attentions.append(attention.squeeze(1).cpu().numpy())\n",
    "        \n",
    "        pred_token = output.argmax(1).item()\n",
    "        \n",
    "        # Stop if EOS\n",
    "        if pred_token == trg_vocab['<EOS>']:\n",
    "            break\n",
    "        \n",
    "        trg_idx.append(pred_token)\n",
    "    \n",
    "    # Convert indices to tokens\n",
    "    trg_tokens = [trg_vocab[i] for i in range(len(trg_vocab)) if i in trg_idx]\n",
    "    \n",
    "    # Remove special tokens\n",
    "    trg_tokens = [token for token in trg_tokens if token not in ['<SOS>', '<EOS>', '<PAD>', '<UNK>']]\n",
    "    \n",
    "    return ''.join(trg_tokens), attentions\n",
    "\n",
    "def calculate_sequence_accuracy(predictions, targets):\n",
    "    correct = 0\n",
    "    total = len(targets)\n",
    "    \n",
    "    for pred, target in zip(predictions, targets):\n",
    "        if pred == target:\n",
    "            correct += 1\n",
    "    \n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# # Cell 3 correction: Fix translate_sentence function\n",
    "# def translate_sentence(sentence, src_vocab, trg_vocab, model, device, max_len=100):\n",
    "#     model.eval()\n",
    "    \n",
    "#     # Convert sentence to indices\n",
    "#     tokens = [src_vocab.get(char, src_vocab['<UNK>']) for char in sentence]\n",
    "#     tokens.append(src_vocab['<EOS>'])\n",
    "    \n",
    "#     src_tensor = torch.LongTensor(tokens).unsqueeze(0).to(device)\n",
    "#     src_lens = torch.LongTensor([len(tokens)])\n",
    "    \n",
    "#     # Create mask for encoder outputs\n",
    "#     mask = model.create_mask(src_tensor, src_lens)\n",
    "    \n",
    "#     with torch.no_grad():\n",
    "#         if model.encoder.cell_type == 'LSTM':\n",
    "#             encoder_outputs, hidden, cell = model.encoder(src_tensor, src_lens)\n",
    "#         else:\n",
    "#             encoder_outputs, hidden = model.encoder(src_tensor, src_lens)\n",
    "#             cell = None\n",
    "    \n",
    "#     # First token is SOS\n",
    "#     trg_idx = [trg_vocab['< SOS >']]\n",
    "#     attentions = []\n",
    "    \n",
    "#     for i in range(max_len):\n",
    "#         trg_tensor = torch.LongTensor([trg_idx[-1]]).to(device)\n",
    "        \n",
    "#         with torch.no_grad():\n",
    "#             if model.decoder.cell_type == 'LSTM':\n",
    "#                 output, hidden, cell, attention = model.decoder(trg_tensor, hidden, encoder_outputs, cell, mask)\n",
    "#             else:\n",
    "#                 output, hidden, attention = model.decoder(trg_tensor, hidden, encoder_outputs, mask=mask)\n",
    "                \n",
    "#         attentions.append(attention.squeeze(1).cpu().numpy())\n",
    "        \n",
    "#         pred_token = output.argmax(1).item()\n",
    "        \n",
    "#         # Stop if EOS\n",
    "#         if pred_token == trg_vocab['<EOS>']:\n",
    "#             break\n",
    "        \n",
    "#         trg_idx.append(pred_token)\n",
    "    \n",
    "#     # Create inverse vocab dictionary if not provided\n",
    "#     trg_inv_vocab = {v: k for k, v in trg_vocab.items()}\n",
    "    \n",
    "#     # Convert indices to tokens using inverse vocab\n",
    "#     trg_tokens = [trg_inv_vocab.get(idx, '<UNK>') for idx in trg_idx]\n",
    "    \n",
    "#     # Remove special tokens\n",
    "#     trg_tokens = [token for token in trg_tokens if token not in ['< SOS >', '<EOS>', '<PAD>', '<UNK>']]\n",
    "    \n",
    "#     return ''.join(trg_tokens), attentions\n",
    "\n",
    "def translate_sentence(sentence, src_vocab, trg_vocab, model, device, max_len=100):\n",
    "    model.eval()\n",
    "    \n",
    "    # Convert sentence to indices\n",
    "    tokens = [src_vocab.get(char, src_vocab['<UNK>']) for char in sentence]\n",
    "    tokens.append(src_vocab['<EOS>'])\n",
    "    \n",
    "    src_tensor = torch.LongTensor(tokens).unsqueeze(0).to(device)\n",
    "    src_lens = torch.LongTensor([len(tokens)])\n",
    "    \n",
    "    # Create mask for encoder outputs\n",
    "    mask = model.create_mask(src_tensor, src_lens)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        if model.encoder.cell_type == 'LSTM':\n",
    "            encoder_outputs, hidden, cell = model.encoder(src_tensor, src_lens)\n",
    "        else:\n",
    "            encoder_outputs, hidden = model.encoder(src_tensor, src_lens)\n",
    "            cell = None\n",
    "    \n",
    "    # First token is SOS\n",
    "    trg_idx = [trg_vocab['<SOS>']]\n",
    "    attentions = []\n",
    "    \n",
    "    for i in range(max_len):\n",
    "        trg_tensor = torch.LongTensor([trg_idx[-1]]).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            if model.decoder.cell_type == 'LSTM':\n",
    "                output, hidden, cell, attention = model.decoder(trg_tensor, hidden, encoder_outputs, cell, mask)\n",
    "            else:\n",
    "                output, hidden, attention = model.decoder(trg_tensor, hidden, encoder_outputs, mask=mask)\n",
    "                \n",
    "        attentions.append(attention.squeeze(1).cpu().numpy())\n",
    "        \n",
    "        pred_token = output.argmax(1).item()\n",
    "        \n",
    "        # Stop if EOS\n",
    "        if pred_token == trg_vocab['<EOS>']:\n",
    "            break\n",
    "        \n",
    "        trg_idx.append(pred_token)\n",
    "    \n",
    "    results = load_data(train_path='/kaggle/input/hindi-dl/hi.translit.sampled.train.tsv', dev_path='/kaggle/input/hindi-dl/hi.translit.sampled.dev.tsv', test_path='/kaggle/input/hindi-dl/hi.translit.sampled.test.tsv')\n",
    "    # Convert indices to tokens\n",
    "    trg_inv_vocab = {v: k for k, v in trg_vocab.items()}\n",
    "    trg_tokens = [results['devanagari_inv_vocab'].get(i, '<UNK>') for i in trg_idx]\n",
    "    \n",
    "    # Remove special tokens\n",
    "    trg_tokens = [token for token in trg_tokens if token not in ['<SOS>', '<EOS>', '<PAD>', '<UNK>']]\n",
    "    \n",
    "    return ''.join(trg_tokens), attentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Cell 4: Training, evaluation, and testing functions\n",
    "def train(model, iterator, optimizer, criterion, teacher_forcing_ratio, clip, device):\n",
    "    model.train()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for batch in iterator:\n",
    "        src = batch['latin'].to(device)\n",
    "        trg = batch['devanagari'].to(device)\n",
    "        src_lens = batch['latin_lens']\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output, _ = model(src, src_lens, trg, teacher_forcing_ratio)\n",
    "        \n",
    "        # Exclude the first token (SOS) from the loss calculation\n",
    "        output_dim = output.shape[-1]\n",
    "        output = output[:, 1:].reshape(-1, output_dim)\n",
    "        trg = trg[:, 1:].reshape(-1)\n",
    "        \n",
    "        loss = criterion(output, trg)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        # Clip gradients to prevent exploding gradients\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "    return epoch_loss / len(iterator)\n",
    "\n",
    "def evaluate(model, iterator, criterion, device):\n",
    "    model.eval()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            src = batch['latin'].to(device)\n",
    "            trg = batch['devanagari'].to(device)\n",
    "            src_lens = batch['latin_lens']\n",
    "            \n",
    "            output, _ = model(src, src_lens, trg, 0)  # No teacher forcing during evaluation\n",
    "            \n",
    "            # Exclude the first token (SOS) from the loss calculation\n",
    "            output_dim = output.shape[-1]\n",
    "            output = output[:, 1:].reshape(-1, output_dim)\n",
    "            trg = trg[:, 1:].reshape(-1)\n",
    "            \n",
    "            loss = criterion(output, trg)\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "            # Calculate sequence accuracy\n",
    "            output = output.view(batch['devanagari'].shape[0], -1, output_dim)\n",
    "            predictions = output.argmax(2)\n",
    "            \n",
    "            # Compare predictions with targets (excluding padding)\n",
    "            for i in range(len(batch['devanagari_texts'])):\n",
    "                pred_seq = []\n",
    "                for j in range(1, batch['devanagari_lens'][i] - 1):  # Exclude SOS and EOS\n",
    "                    pred_token = predictions[i, j-1].item()\n",
    "                    if pred_token != 0:  # Not PAD\n",
    "                        pred_seq.append(pred_token)\n",
    "                \n",
    "                true_seq = []\n",
    "                for j in range(1, batch['devanagari_lens'][i] - 1):  # Exclude SOS and EOS\n",
    "                    true_token = trg[i * (batch['devanagari_lens'].max().item() - 1) + (j-1)].item()\n",
    "                    if true_token != 0:  # Not PAD\n",
    "                        true_seq.append(true_token)\n",
    "                \n",
    "                if len(pred_seq) == len(true_seq) and all(p == t for p, t in zip(pred_seq, true_seq)):\n",
    "                    correct_predictions += 1\n",
    "                    \n",
    "                total_predictions += 1\n",
    "    \n",
    "    accuracy = correct_predictions / total_predictions if total_predictions > 0 else 0\n",
    "    \n",
    "    return epoch_loss / len(iterator), accuracy\n",
    "\n",
    "# def translate_sentence(sentence, src_vocab, trg_vocab, model, device, max_len=100):\n",
    "#     model.eval()\n",
    "    \n",
    "#     # Convert sentence to indices\n",
    "#     tokens = [src_vocab.get(char, src_vocab['<UNK>']) for char in sentence]\n",
    "#     tokens.append(src_vocab['<EOS>'])\n",
    "    \n",
    "#     src_tensor = torch.LongTensor(tokens).unsqueeze(0).to(device)\n",
    "#     src_lens = torch.LongTensor([len(tokens)])\n",
    "    \n",
    "#     # Create mask for encoder outputs\n",
    "#     mask = model.create_mask(src_tensor, src_lens)\n",
    "    \n",
    "#     with torch.no_grad():\n",
    "#         if model.encoder.cell_type == 'LSTM':\n",
    "#             encoder_outputs, hidden, cell = model.encoder(src_tensor, src_lens)\n",
    "#         else:\n",
    "#             encoder_outputs, hidden = model.encoder(src_tensor, src_lens)\n",
    "#             cell = None\n",
    "    \n",
    "    # # First token is SOS\n",
    "    # trg_idx = [trg_vocab['<SOS>']]\n",
    "    # attentions = []\n",
    "    \n",
    "    # for i in range(max_len):\n",
    "    #     trg_tensor = torch.LongTensor([trg_idx[-1]]).to(device)\n",
    "        \n",
    "    #     with torch.no_grad():\n",
    "    #         if model.decoder.cell_type == 'LSTM':\n",
    "    #             output, hidden, cell, attention = model.decoder(trg_tensor, hidden, encoder_outputs, cell, mask)\n",
    "    #         else:\n",
    "    #             output, hidden, attention = model.decoder(trg_tensor, hidden, encoder_outputs, mask=mask)\n",
    "                \n",
    "    #     attentions.append(attention.squeeze(1).cpu().numpy())\n",
    "        \n",
    "    #     pred_token = output.argmax(1).item()\n",
    "        \n",
    "    #     # Stop if EOS\n",
    "    #     if pred_token == trg_vocab['<EOS>']:\n",
    "    #         break\n",
    "        \n",
    "    #     trg_idx.append(pred_token)\n",
    "    \n",
    "    # Convert indices to tokens\n",
    "    # trg_tokens = [trg_vocab[i] for i in range(len(trg_vocab)) if i in trg_idx]\n",
    "    # trg_tokens = [trg_inv_vocab.get(i, '<UNK>') for i in trg_idx]\n",
    "\n",
    "    # # Remove special tokens\n",
    "    # trg_tokens = [token for token in trg_tokens if token not in ['<SOS>', '<EOS>', '<PAD>', '<UNK>']]\n",
    "    \n",
    "    # return ''.join(trg_tokens), attentions\n",
    "\n",
    "def calculate_sequence_accuracy(predictions, targets):\n",
    "    correct = 0\n",
    "    total = len(targets)\n",
    "    \n",
    "    for pred, target in zip(predictions, targets):\n",
    "        if pred == target:\n",
    "            correct += 1\n",
    "    \n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Cell 5: WandB sweep function\n",
    "def wandb_sweep_train():\n",
    "    # Initialize wandb\n",
    "    wandb.init()\n",
    "    \n",
    "    # Get hyperparameters from wandb\n",
    "    config = wandb.config\n",
    "    \n",
    "    # Set device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # Load data\n",
    "    data = load_data('/kaggle/input/hindi-dl/hi.translit.sampled.train.tsv', \n",
    "                    '/kaggle/input/hindi-dl/hi.translit.sampled.dev.tsv', \n",
    "                    '/kaggle/input/hindi-dl/hi.translit.sampled.test.tsv')\n",
    "    \n",
    "    train_dataset = data['train_dataset']\n",
    "    dev_dataset = data['dev_dataset']\n",
    "    latin_vocab = data['latin_vocab']\n",
    "    devanagari_vocab = data['devanagari_vocab']\n",
    "    \n",
    "    # Create dataloaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=config.batch_size, collate_fn=collate_fn, shuffle=True)\n",
    "    dev_loader = DataLoader(dev_dataset, batch_size=config.batch_size, collate_fn=collate_fn)\n",
    "    \n",
    "    # Set model parameters\n",
    "    input_dim = len(latin_vocab)\n",
    "    output_dim = len(devanagari_vocab)\n",
    "    \n",
    "    # Initialize attention\n",
    "    attention = Attention(config.hidden_dim)\n",
    "    \n",
    "    # Initialize encoder and decoder\n",
    "    encoder = Encoder(input_dim, config.emb_dim, config.hidden_dim, config.enc_layers, config.dropout, config.cell_type)\n",
    "    decoder = Decoder(output_dim, config.emb_dim, config.hidden_dim, config.dec_layers, config.dropout, config.cell_type, attention)\n",
    "    \n",
    "    # Initialize the model\n",
    "    model = Seq2SeqWithAttention(encoder, decoder, device).to(device)\n",
    "    \n",
    "    # Print model parameters\n",
    "    print(f'The model has {sum(p.numel() for p in model.parameters() if p.requires_grad):,} trainable parameters')\n",
    "    \n",
    "    # Initialize optimizer and criterion\n",
    "    optimizer = optim.Adam(model.parameters(), lr=config.learning_rate)\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=0)  # Ignore padding\n",
    "    \n",
    "    # Initialize early stopping variables\n",
    "    best_valid_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    \n",
    "    for epoch in range(config.epochs):\n",
    "        # Train\n",
    "        train_loss = train(model, train_loader, optimizer, criterion, config.teacher_forcing, 1.0, device)\n",
    "        \n",
    "        # Evaluate\n",
    "        valid_loss, valid_acc = evaluate(model, dev_loader, criterion, device)\n",
    "        \n",
    "        # Log metrics\n",
    "        wandb.log({\n",
    "            'train_loss': train_loss,\n",
    "            'val_loss': valid_loss,\n",
    "            'val_sequence_accuracy': valid_acc,\n",
    "            'epoch': epoch\n",
    "        })\n",
    "        \n",
    "        print(f'Epoch: {epoch+1}')\n",
    "        print(f'\\tTrain Loss: {train_loss:.3f}')\n",
    "        print(f'\\tValid Loss: {valid_loss:.3f}')\n",
    "        print(f'\\tValid Acc: {valid_acc:.3f}')\n",
    "        \n",
    "        # Early stopping\n",
    "        if valid_loss < best_valid_loss:\n",
    "            best_valid_loss = valid_loss\n",
    "            patience_counter = 0\n",
    "            \n",
    "            # Save the best model\n",
    "            torch.save(model.state_dict(), 'best_model.pt')\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            \n",
    "        if patience_counter >= config.patience:\n",
    "            print(\"Early stopping!\")\n",
    "            break\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Cell 6: Main execution code\n",
    "def main():\n",
    "    # Initialize wandb\n",
    "    wandb.login()\n",
    "    \n",
    "    # Define sweep configuration\n",
    "    sweep_config = {\n",
    "        'method': 'bayes',\n",
    "        'metric': {'name': 'val_sequence_accuracy', 'goal': 'maximize'},\n",
    "        'parameters': {\n",
    "            'emb_dim': {'values': [64, 128, 256]},\n",
    "            'hidden_dim': {'values': [128, 256]},\n",
    "            'enc_layers': {'values': [1, 2, 3]},\n",
    "            'dec_layers': {'values': [1, 2, 3]},\n",
    "            'cell_type': {'values': ['LSTM', 'GRU', 'RNN']},\n",
    "            'dropout': {'values': [0.2, 0.3, 0.4]},\n",
    "            'batch_size': {'values': [32, 64, 128]},\n",
    "            'learning_rate': {'values': [0.001, 0.0005, 0.0001]},\n",
    "            'teacher_forcing': {'values': [0.5, 0.7, 0.9]},\n",
    "            'patience': {'value': 3},\n",
    "            'epochs': {'values': [10, 15]}\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Create sweep\n",
    "    sweep_id = wandb.sweep(sweep_config, project=\"hindi-transliteration-attention\")\n",
    "    \n",
    "    # Run sweep\n",
    "    wandb.agent(sweep_id, wandb_sweep_train, count=15)  # Run 15 experiments\n",
    "\n",
    "# Uncomment to run the sweep\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def evaluate_best_model():\n",
    "    # Load data\n",
    "    data = load_data('/kaggle/input/hindi-dl/hi.translit.sampled.train.tsv', \n",
    "                     '/kaggle/input/hindi-dl/hi.translit.sampled.dev.tsv', \n",
    "                     '/kaggle/input/hindi-dl/hi.translit.sampled.test.tsv')\n",
    " \n",
    "    test_dataset = data['test_dataset']\n",
    "    latin_vocab = data['latin_vocab']\n",
    "    devanagari_vocab = data['devanagari_vocab']\n",
    "    latin_inv_vocab = data['latin_inv_vocab']\n",
    "    devanagari_inv_vocab = data['devanagari_inv_vocab']\n",
    "    \n",
    "    # Create dataloaders\n",
    "    test_loader = DataLoader(test_dataset, batch_size=64, collate_fn=collate_fn)\n",
    "    \n",
    "    # Set device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # Load the best hyperparameters (assume these are from the sweep)\n",
    "    best_config = {\n",
    "        'emb_dim': 256,\n",
    "        'hidden_dim': 256,\n",
    "        'enc_layers': 2,\n",
    "        'dec_layers': 2,\n",
    "        'cell_type': 'LSTM',\n",
    "        'dropout': 0.3\n",
    "    }\n",
    "    \n",
    "    # Set model parameters\n",
    "    input_dim = len(latin_vocab)\n",
    "    output_dim = len(devanagari_vocab)\n",
    "    \n",
    "    # Initialize attention\n",
    "    attention = Attention(best_config['hidden_dim'])\n",
    "    \n",
    "    # Initialize encoder and decoder\n",
    "    encoder = Encoder(input_dim, best_config['emb_dim'], best_config['hidden_dim'], \n",
    "                     best_config['enc_layers'], best_config['dropout'], best_config['cell_type'])\n",
    "    decoder = Decoder(output_dim, best_config['emb_dim'], best_config['hidden_dim'], \n",
    "                     best_config['dec_layers'], best_config['dropout'], best_config['cell_type'], attention)\n",
    "    \n",
    "    # Initialize the model\n",
    "    model = Seq2SeqWithAttention(encoder, decoder, device).to(device)\n",
    "    \n",
    "    # Load the best model\n",
    "    model.load_state_dict(torch.load('best_model.pt'))\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=0)  # Ignore padding\n",
    "    test_loss, test_accuracy = evaluate(model, test_loader, criterion, device)\n",
    "    \n",
    "    print(f'Test Loss: {test_loss:.3f}')\n",
    "    print(f'Test Accuracy: {test_accuracy:.3f}')\n",
    "    \n",
    "    # Generate predictions for test samples\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "    all_inputs = []\n",
    "    attention_maps = []\n",
    "    \n",
    "    # Get a sample of test data for visualization\n",
    "    test_samples = []\n",
    "    for i, item in enumerate(test_dataset):\n",
    "        if i < 9:  # Get 9 samples for visualization\n",
    "            latin_text = item['latin_text']\n",
    "            devanagari_text = item['devanagari_text']\n",
    "            test_samples.append((latin_text, devanagari_text))\n",
    "    \n",
    "    # Generate predictions and attention maps\n",
    "    for latin_text, devanagari_text in test_samples:\n",
    "        prediction, attentions = translate_sentence(latin_text, latin_vocab, devanagari_vocab, model, device)\n",
    "        \n",
    "        all_inputs.append(latin_text)\n",
    "        all_predictions.append(prediction)\n",
    "        all_targets.append(devanagari_text)\n",
    "        attention_maps.append((latin_text, prediction, devanagari_text, attentions))\n",
    "    \n",
    "    # Save predictions to CSV file\n",
    "    import csv\n",
    "    with open('predictions_attention.csv', 'w', encoding='utf-8', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        # Write header\n",
    "        writer.writerow(['Input', 'Prediction', 'Target'])\n",
    "        # Write data\n",
    "        for inp, pred, target in zip(all_inputs, all_predictions, all_targets):\n",
    "            writer.writerow([inp, pred, target])\n",
    "    \n",
    "    # Visualize attention maps\n",
    "    visualize_attention_maps(attention_maps)\n",
    "    \n",
    "    return test_accuracy, all_predictions, all_targets\n",
    "\n",
    "def visualize_attention_maps(attention_maps):\n",
    "    \"\"\"\n",
    "    Visualize attention maps for test samples.\n",
    "    \n",
    "    Args:\n",
    "        attention_maps: List of tuples (latin_text, prediction, devanagari_text, attentions)\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(3, 3, figsize=(15, 15))\n",
    "    \n",
    "    for i, (latin_text, prediction, devanagari_text, attentions) in enumerate(attention_maps[:9]):\n",
    "        row = i // 3\n",
    "        col = i % 3\n",
    "        \n",
    "        # Get attention for the first token (we'll just use the average of all attentions)\n",
    "        attention = np.mean(np.array(attentions), axis=0)\n",
    "        \n",
    "        # Plot heatmap\n",
    "        sns.heatmap(attention, xticklabels=list(latin_text) + ['<EOS>'], \n",
    "                   yticklabels=['<AVG>'], ax=axes[row, col], cmap='viridis')\n",
    "        \n",
    "        axes[row, col].set_title(f\"Input: {latin_text}\\nPred: {prediction}\\nTarget: {devanagari_text}\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('attention_maps.png')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def compare_models():\n",
    "    \"\"\"Compare attention-based model with vanilla model.\"\"\"\n",
    "    import csv\n",
    "    \n",
    "    # Try to load vanilla model predictions\n",
    "    try:\n",
    "        vanilla_preds = []\n",
    "        vanilla_targets = []\n",
    "        attention_preds = []\n",
    "        attention_targets = []\n",
    "        inputs = []\n",
    "        \n",
    "        # Parse vanilla predictions\n",
    "        with open('/kaggle/input/prediction-vanilla/test_predictions.csv', 'r', encoding='utf-8') as f:\n",
    "            csv_reader = csv.reader(f)\n",
    "            header = next(csv_reader, None)  # Skip header if exists\n",
    "            for row in csv_reader:\n",
    "                if len(row) >= 3:  # Assuming CSV format: input, prediction, target\n",
    "                    inputs.append(row[0])\n",
    "                    vanilla_preds.append(row[1])\n",
    "                    vanilla_targets.append(row[2])\n",
    "        \n",
    "        # Parse attention predictions\n",
    "        with open('/kaggle/working/predictions_attention.csv', 'r', encoding='utf-8') as f:\n",
    "            csv_reader = csv.reader(f)\n",
    "            header = next(csv_reader, None)  # Skip header if exists\n",
    "            for row in csv_reader:\n",
    "                if len(row) >= 3:  # Assuming CSV format: input, prediction, target\n",
    "                    attention_preds.append(row[1])\n",
    "                    attention_targets.append(row[2])\n",
    "        \n",
    "        # Calculate accuracies\n",
    "        vanilla_correct = sum(1 for p, t in zip(vanilla_preds, vanilla_targets) if p == t)\n",
    "        attention_correct = sum(1 for p, t in zip(attention_preds, attention_targets) if p == t)\n",
    "        \n",
    "        vanilla_acc = vanilla_correct / len(vanilla_targets) if vanilla_targets else 0\n",
    "        attention_acc = attention_correct / len(attention_targets) if attention_targets else 0\n",
    "        \n",
    "        print(f\"Vanilla Accuracy: {vanilla_acc:.4f}\")\n",
    "        print(f\"Attention Accuracy: {attention_acc:.4f}\")\n",
    "        \n",
    "        # Find examples that attention model corrects\n",
    "        corrected_examples = []\n",
    "        for i, (v_pred, a_pred, target, inp) in enumerate(zip(vanilla_preds, attention_preds, attention_targets, inputs)):\n",
    "            if v_pred != target and a_pred == target:\n",
    "                corrected_examples.append((inp, v_pred, a_pred, target))\n",
    "        \n",
    "        # print(f\"\\nExamples corrected by attention model ({len(corrected_examples)}):\")\n",
    "        for i, (inp, v_pred, a_pred, target) in enumerate(corrected_examples[:5]):  # Show at most 5 examples\n",
    "            # print(f\"Example {i+1}:\")\n",
    "            # print(f\"  Input: {inp}\")\n",
    "            # print(f\"  Vanilla prediction: {v_pred}\")\n",
    "            # print(f\"  Attention prediction: {a_pred}\")\n",
    "            # print(f\"  Target: {target}\")\n",
    "            print()\n",
    "        \n",
    "        return vanilla_acc, attention_acc, corrected_examples\n",
    "    except FileNotFoundError:\n",
    "        print()\n",
    "        return None, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Cell 9: Function to run a single experiment\n",
    "def run_single_experiment():\n",
    "    \"\"\"Run a single experiment with best hyperparameters.\"\"\"\n",
    "    # Initialize wandb\n",
    "    wandb.init(project=\"hindi-transliteration-attention\", name=\"single_experiment\")\n",
    "    \n",
    "    # Set device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # Load data\n",
    "    data = load_data('/kaggle/input/hindi-dl/hi.translit.sampled.train.tsv', \n",
    "                    '/kaggle/input/hindi-dl/hi.translit.sampled.dev.tsv', \n",
    "                    '/kaggle/input/hindi-dl/hi.translit.sampled.test.tsv')\n",
    "    \n",
    "    \n",
    "    train_dataset = data['train_dataset']\n",
    "    dev_dataset = data['dev_dataset']\n",
    "    latin_vocab = data['latin_vocab']\n",
    "    devanagari_vocab = data['devanagari_vocab']\n",
    "    \n",
    "    # Create dataloaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=64, collate_fn=collate_fn, shuffle=True)\n",
    "    dev_loader = DataLoader(dev_dataset, batch_size=64, collate_fn=collate_fn)\n",
    "    \n",
    "    # Set model parameters\n",
    "    input_dim = len(latin_vocab)\n",
    "    output_dim = len(devanagari_vocab)\n",
    "    \n",
    "    # Best hyperparameters\n",
    "    config = {\n",
    "        'emb_dim': 256,\n",
    "        'hidden_dim': 256,\n",
    "        'enc_layers': 2,\n",
    "        'dec_layers': 2,\n",
    "        'cell_type': 'LSTM',\n",
    "        'dropout': 0.3,\n",
    "        'learning_rate': 0.001,\n",
    "        'teacher_forcing': 0.7,\n",
    "        'patience': 3,\n",
    "        'epochs': 15\n",
    "    }\n",
    "    \n",
    "    # Initialize attention\n",
    "    attention = Attention(config['hidden_dim'])\n",
    "    \n",
    "    # Initialize encoder and decoder\n",
    "    encoder = Encoder(input_dim, config['emb_dim'], config['hidden_dim'], \n",
    "                     config['enc_layers'], config['dropout'], config['cell_type'])\n",
    "    decoder = Decoder(output_dim, config['emb_dim'], config['hidden_dim'], \n",
    "                     config['dec_layers'], config['dropout'], config['cell_type'], attention)\n",
    "    \n",
    "    # Initialize the model\n",
    "    model = Seq2SeqWithAttention(encoder, decoder, device).to(device)\n",
    "    \n",
    "    # Print model parameters\n",
    "    print(f'The model has {sum(p.numel() for p in model.parameters() if p.requires_grad):,} trainable parameters')\n",
    "    \n",
    "    # Initialize optimizer and criterion\n",
    "    optimizer = optim.Adam(model.parameters(), lr=config['learning_rate'])\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=0)  # Ignore padding\n",
    "    \n",
    "    # Initialize early stopping variables\n",
    "    best_valid_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    \n",
    "    for epoch in range(config['epochs']):\n",
    "        # Train\n",
    "        train_loss = train(model, train_loader, optimizer, criterion, config['teacher_forcing'], 1.0, device)\n",
    "        \n",
    "        # Evaluate\n",
    "        valid_loss, valid_acc = evaluate(model, dev_loader, criterion, device)\n",
    "        \n",
    "        # Log metrics\n",
    "        wandb.log({\n",
    "            'train_loss': train_loss,\n",
    "            'val_loss': valid_loss,\n",
    "            'val_sequence_accuracy': valid_acc,\n",
    "            'epoch': epoch\n",
    "        })\n",
    "        \n",
    "        print(f'Epoch: {epoch+1}')\n",
    "        print(f'\\tTrain Loss: {train_loss:.3f}')\n",
    "        print(f'\\tValid Loss: {valid_loss:.3f}')\n",
    "        print(f'\\tValid Acc: {valid_acc:.3f}')\n",
    "        \n",
    "        # Early stopping\n",
    "        if valid_loss < best_valid_loss:\n",
    "            best_valid_loss = valid_loss\n",
    "            patience_counter = 0\n",
    "            \n",
    "            # Save the best model\n",
    "            torch.save(model.state_dict(), 'best_model.pt')\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            \n",
    "        if patience_counter >= config['patience']:\n",
    "            print(\"Early stopping!\")\n",
    "            break\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Cell 10: Main function\n",
    "def main():\n",
    "    # Choose whether to run sweep or single experiment\n",
    "    run_sweep = False  # Set to True to run sweep, False to run single experiment\n",
    "    \n",
    "    if run_sweep:\n",
    "        print(\"Running hyperparameter sweep...\")\n",
    "        # Initialize wandb\n",
    "        wandb.login()\n",
    "        \n",
    "        # Define sweep configuration\n",
    "        sweep_config = {\n",
    "            'method': 'bayes',\n",
    "            'metric': {'name': 'val_sequence_accuracy', 'goal': 'maximize'},\n",
    "            'parameters': {\n",
    "                'emb_dim': {'values': [64, 128, 256]},\n",
    "                'hidden_dim': {'values': [128, 256]},\n",
    "                'enc_layers': {'values': [1, 2, 3]},\n",
    "                'dec_layers': {'values': [1, 2, 3]},\n",
    "                'cell_type': {'values': ['LSTM', 'GRU', 'RNN']},\n",
    "                'dropout': {'values': [0.2, 0.3, 0.4]},\n",
    "                'batch_size': {'values': [32, 64, 128]},\n",
    "                'learning_rate': {'values': [0.001, 0.0005, 0.0001]},\n",
    "                'teacher_forcing': {'values': [0.5, 0.7, 0.9]},\n",
    "                'patience': {'value': 3},\n",
    "                'epochs': {'values': [10, 15]}\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Create sweep\n",
    "        sweep_id = wandb.sweep(sweep_config, project=\"hindi-transliteration-attention\")\n",
    "        \n",
    "        # Run sweep\n",
    "        wandb.agent(sweep_id, wandb_sweep_train, count=15)  # Run 15 experiments\n",
    "    else:\n",
    "        print(\"Running single experiment...\")\n",
    "        # Run single experiment\n",
    "        model = run_single_experiment()\n",
    "    \n",
    "    # Evaluate best model\n",
    "    print(\"\\nEvaluating best model on test set...\")\n",
    "    accuracy, predictions, targets = evaluate_best_model()\n",
    "    \n",
    "    # Compare with vanilla model\n",
    "    print(\"\\nComparing with vanilla model...\")\n",
    "    vanilla_acc, attention_acc, corrected_examples = compare_models()\n",
    "    \n",
    "    print(f\"\\nAttention Model Test Accuracy: {accuracy:.4f}\")\n",
    "    if vanilla_acc is not None:\n",
    "        print(f\"Improvement over Vanilla Model: {(attention_acc - vanilla_acc)*100:.2f}%\")\n",
    "    \n",
    "    # Display WandB report link\n",
    "    print(\"\\nCheck WandB for detailed reports and visualizations.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Cell 11: Connectivity visualization (Q6)\n",
    "def plot_connectivity(input_sentence, output_sentence, attention_weights):\n",
    "    \"\"\"\n",
    "    Plot connectivity visualization as mentioned in Q6.\n",
    "    This shows which input character the model is attending to when decoding each output character.\n",
    "    \n",
    "    Args:\n",
    "        input_sentence: Input string in Latin\n",
    "        output_sentence: Output string in Devanagari\n",
    "        attention_weights: Attention weights from the model\n",
    "    \"\"\"\n",
    "    # Create figure\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Remove first attention weight (for SOS token)\n",
    "    attention_weights = np.concatenate(attention_weights[1:], axis=0)\n",
    "    \n",
    "    # Get the maximum attention weight for each output character\n",
    "    max_attention_indices = np.argmax(attention_weights, axis=1)\n",
    "    \n",
    "    # Plot input and output characters\n",
    "    input_x = np.arange(len(input_sentence))\n",
    "    output_y = np.arange(len(output_sentence))\n",
    "    \n",
    "    # Plot input characters\n",
    "    for i, char in enumerate(input_sentence):\n",
    "        plt.text(i, -0.2, char, fontsize=14, ha='center')\n",
    "    \n",
    "    # Plot output characters\n",
    "    for i, char in enumerate(output_sentence):\n",
    "        plt.text(-0.5, i, char, fontsize=14, va='center')\n",
    "    \n",
    "    # Plot connections\n",
    "    for i in range(len(output_sentence)):\n",
    "        j = max_attention_indices[i]\n",
    "        weight = attention_weights[i, j]\n",
    "        \n",
    "        # Draw line with alpha proportional to weight\n",
    "        plt.plot([j, -0.2], [i, i], 'r-', alpha=weight, linewidth=2)\n",
    "        \n",
    "    # Set axis limits\n",
    "    plt.xlim(-1, len(input_sentence))\n",
    "    plt.ylim(-1, len(output_sentence))\n",
    "    \n",
    "    # Remove axis ticks\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    \n",
    "    # Add title\n",
    "    plt.title('Connectivity: Which input character is attended to when decoding each output character?')\n",
    "    \n",
    "    # Save figure\n",
    "    plt.savefig('predictions_attention/connectivity_visualization.png')\n",
    "    plt.close()\n",
    "\n",
    "def generate_connectivity_visualizations():\n",
    "    \"\"\"\n",
    "    Generate connectivity visualizations for 5 examples\n",
    "    \"\"\"\n",
    "    # Load data\n",
    "    data = load_data('/kaggle/input/hindi-dl/hi.translit.sampled.train.tsv', \n",
    "                    '/kaggle/input/hindi-dl/hi.translit.sampled.dev.tsv', \n",
    "                    '/kaggle/input/hindi-dl/hi.translit.sampled.test.tsv')\n",
    "    \n",
    "    test_dataset = data['test_dataset']\n",
    "    latin_vocab = data['latin_vocab']\n",
    "    devanagari_vocab = data['devanagari_vocab']\n",
    "    \n",
    "    # Set device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # Load model configuration\n",
    "    config = {\n",
    "        'emb_dim': 256,\n",
    "        'hidden_dim': 256,\n",
    "        'enc_layers': 2,\n",
    "        'dec_layers': 2,\n",
    "        'cell_type': 'LSTM',\n",
    "        'dropout': 0.3\n",
    "    }\n",
    "    \n",
    "    # Set model parameters\n",
    "    input_dim = len(latin_vocab)\n",
    "    output_dim = len(devanagari_vocab)\n",
    "    \n",
    "    # Initialize attention\n",
    "    attention = Attention(config['hidden_dim'])\n",
    "    \n",
    "    # Initialize encoder and decoder\n",
    "    encoder = Encoder(input_dim, config['emb_dim'], config['hidden_dim'], \n",
    "                     config['enc_layers'], config['dropout'], config['cell_type'])\n",
    "    decoder = Decoder(output_dim, config['emb_dim'], config['hidden_dim'], \n",
    "                     config['dec_layers'], config['dropout'], config['cell_type'], attention)\n",
    "    \n",
    "    # Initialize the model\n",
    "    model = Seq2SeqWithAttention(encoder, decoder, device).to(device)\n",
    "    \n",
    "    # Load the best model\n",
    "    model.load_state_dict(torch.load('best_model.pt'))\n",
    "    \n",
    "    # Sample 5 examples for visualization\n",
    "    sample_indices = random.sample(range(len(test_dataset)), 5)\n",
    "    \n",
    "    for i, idx in enumerate(sample_indices):\n",
    "        item = test_dataset[idx]\n",
    "        latin_text = item['latin_text']\n",
    "        target = item['devanagari_text']\n",
    "        \n",
    "        # Translate\n",
    "        prediction, attentions = translate_sentence(latin_text, latin_vocab, devanagari_vocab, model, device)\n",
    "        \n",
    "        # Plot connectivity\n",
    "        plot_connectivity(latin_text, prediction, attentions)\n",
    "        \n",
    "        print(f\"Example {i+1}:\")\n",
    "        print(f\"  Latin: {latin_text}\")\n",
    "        print(f\"  Predicted: {prediction}\")\n",
    "        print(f\"  Target: {target}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 7462499,
     "sourceId": 11874346,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7471396,
     "sourceId": 11887175,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
